![](assets/logo.png?v=1&type=image)
<div align="center">
<h3>Mobile-Agent: å¼ºå¤§çš„ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹å®¶æ—<h3>
<div align="center">
	<a href="https://huggingface.co/spaces/junyangwang0410/Mobile-Agent"><img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg" alt="Open in Spaces"></a>
	<a href="https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2"><img src="assets/Demo-ModelScope-brightgreen.svg" alt="Demo ModelScope"></a>
  <a href="https://arxiv.org/abs/2401.16158"><img src="https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv" alt=""></a>
  <a href="https://arxiv.org/abs/2406.01014 "><img src="https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv" alt=""></a>
</div>
<p align="center">
<a href="https://trendshift.io/repositories/7423" target="_blank"><img src="https://trendshift.io/api/badge/repositories/7423" alt="MobileAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>
</div>

<div align="center">
<a href="README_zh.md">ç®€ä½“ä¸­æ–‡</a> | <a href="README.md">English</a> | <a href="README_ja.md">æ—¥æœ¬èª</a>
<hr>
</div>
<!--
ç®€ä½“ä¸­æ–‡ | [English](README.md) | [æ—¥æœ¬èª](README_ja.md)
<hr>
-->

## ğŸ“ºDemo

### Mobile-Agent-v3ï¼ˆæ³¨æ„ï¼šè¯¥è§†é¢‘æ²¡æœ‰åŠ é€Ÿå¤„ç†ï¼‰
**YouTube**

[![YouTube](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.youtube.com/watch?v=EMbIpzqJld0)

**å“”å“©å“”å“©**

[![Bilibili](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&vd_source=47ffcd57083495a8965c8cdbe1a751ae)

### Mobile-Agent-v2
https://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155

### Mobile-Agent
https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31


## ğŸ“¢æ–°é—»
* ğŸ”¥ğŸ”¥[7.29] Mobile-Agentè·å¾—äº† ***ç¬¬äºŒåä¸‰å±Šä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼š*** (CCL 2024) çš„ **æœ€ä½³demoå¥–é¡¹**ã€‚åœ¨CCL 2024ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†å³å°†å¼€æºçš„Mobile-Agent-v3ï¼Œæ‹¥æœ‰æ›´å°çš„å†…å­˜å¼€é”€ï¼ˆ8 GBï¼‰ã€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆæ¯æ¬¡æ“ä½œ10-15ç§’ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨å¼€æºæ¨¡å‹ã€‚è§†é¢‘Demoè¯·è§ä¸Šä¸€ä¸ªæ¿å—ğŸ“ºDemoã€‚
* ğŸ”¥[6.27] æˆ‘ä»¬åœ¨[Hugging Face](https://huggingface.co/spaces/junyangwang0410/Mobile-Agent)å’Œ[ModelScope](https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2)å‘å¸ƒäº†å¯ä»¥ä¸Šä¼ æ‰‹æœºæˆªå›¾ä½“éªŒMobile-Agent-v2çš„Demoï¼Œæ— éœ€é…ç½®æ¨¡å‹å’Œè®¾å¤‡ï¼Œå³åˆ»ä¾¿å¯ä½“éªŒã€‚
* [6. 4] Modelscope-Agent å·²ç»æ”¯æŒ Mobile-Agent-V2ï¼ŒåŸºäº Android Adb Envï¼Œè¯·æŸ¥çœ‹ [application](https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent)ã€‚
* [6. 4] æˆ‘ä»¬å‘å¸ƒäº†æ–°ä¸€ä»£ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹ Mobile-Agent-v2, é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå®ç°æœ‰æ•ˆå¯¼èˆªã€‚
* [3.10] Mobile-Agent è¢« **ICLR 2024 Workshop on Large Language Model (LLM) Agents** æ¥æ”¶ã€‚

## ğŸ“±ç‰ˆæœ¬
* [Mobile-Agent-v3](Mobile-Agent-v3/README_zh.md)
* [Mobile-Agent-v2](Mobile-Agent-v2/README_zh.md) - é€šè¿‡å¤šä»£ç†åä½œæœ‰æ•ˆå¯¼èˆªçš„ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹
* [Mobile-Agent](Mobile-Agent/README_zh.md) - è§†è§‰æ„ŸçŸ¥æ–¹æ¡ˆçš„è‡ªåŠ¨åŒ–ç§»åŠ¨è®¾å¤‡æ“ä½œæ™ºèƒ½ä½“

## â­Starå†å²
[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&type=Date)](https://star-history.com/#X-PLUG/MobileAgent&Date)

## å¼•ç”¨
If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:
```
@article{wang2024mobile2,
  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},
  author={Wang, Junyang and Xu, Haiyang and Jia Haitao and Zhang Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2406.01014},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}
```

## ğŸ“¦ç›¸å…³é¡¹ç›®
* [AppAgent: Multimodal Agents as Smartphone Users](https://github.com/mnotgod96/AppAgent)
* [mPLUG-Owl & mPLUG-Owl2: Modularized Multimodal Large Language Model](https://github.com/X-PLUG/mPLUG-Owl)
* [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/QwenLM/Qwen-VL)
* [GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://github.com/IDEA-Research/GroundingDINO)
* [CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP)
